{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf131c0e-2e2e-44a1-8777-e4e44b48822e",
   "metadata": {},
   "source": [
    "# LS-SPA Demonstration Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1608a7d-10d9-44ba-bf5b-0d6a5b52ef74",
   "metadata": {},
   "source": [
    "### In this notebook, we use the data from the toy example in Section 2.5 of the paper \"Efficient Shapley Performance Attribution for Least-Squares Regression\" to demonstrate how Shapley values can be computed directly for a linear, least-squares model. We then demonstrate how LS-SPA can be used to generate the same Shapley attribution. In this specific case, we have a very small number of features, so it is feaible to compute the exact Shapley attribution. When the number of features exceeds 15, this is no longer the case. LS-SPA is able to accurately approximate Shapley attributions for linear least-squares models even when the number of features exceeds 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15919105-22a4-4a7b-b406-b118a643d0db",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5dfdd3-d546-4a33-bf5c-1124a87c3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ls_spa import ls_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcb2ec-6ce4-4b6c-8ea1-e8eafce2852b",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e17d28-b1a2-44d6-b1d6-96eb35b37221",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "M = 50\n",
    "p = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74448-7883-4700-8337-91f9578b599a",
   "metadata": {},
   "source": [
    "### The rows of $X$ correspond to observations and the columns of $X$ correspond to features. We fit a least-squares model on the training data `X_train` and `y_train` and evaluate its performance on the test data `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bc0b0b-ec1a-4b99-b5f7-149fb9122dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [np.load(\"./data/toy_data.npz\")[key] for key in [\"X_train\",\"X_test\",\"y_train\",\"y_test\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2aaff5-dc80-4b68-87aa-b7797d05fe1f",
   "metadata": {},
   "source": [
    "## Direct computation of lifts and $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b85fe-fc1a-4da4-be10-505a5fd708cc",
   "metadata": {},
   "source": [
    "### We compute the out-of-sample $R^2$ for a least-squares model fitted on each subset of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec77737-8ca4-408d-bae5-d143e2ad31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = np.zeros((tuple(2 for _ in range(p))))\n",
    "\n",
    "for n in range(2 ** p):\n",
    "    mask = [n // (2 ** i) % 2 for i in range(p)]\n",
    "    indices = list(itertools.compress(range(p), mask))\n",
    "\n",
    "    X_train_sel = X_train[:,indices]\n",
    "    X_test_sel = X_test[:,indices]\n",
    "    theta = np.linalg.lstsq(X_train_sel, y_train, rcond=1e-5)[0]\n",
    "    R2[*mask] = (np.linalg.norm(y_test) ** 2 - np.linalg.norm((X_test_sel @ theta) - y_test) ** 2) / (np.linalg.norm(y_test) ** 2)\n",
    "\n",
    "R2 = np.around(R2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f2ca6-ebab-4889-810c-83c28b0eb058",
   "metadata": {},
   "source": [
    "### For every ordering of our features, we remove one from our model and re-fit sequentially. For each feature, we consider the change in the $R^2$ of the model due to its addition/removal. For a single ordering, the vector of these performance differences due to each feature is a lift vector. The Shapley attribution of our model is the average of the lift vectors for every possible ordering of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2779846a-39cd-4f64-a26c-461c2229357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifts = np.zeros((math.factorial(p), p))\n",
    "perms =  list(itertools.permutations(range(p)))\n",
    "\n",
    "for i, perm in enumerate(perms):\n",
    "    inds = [0,0,0]\n",
    "    perf = R2[*inds]\n",
    "    for lift in perm:\n",
    "        inds[lift] = 1\n",
    "        lifts[i,lift] =  R2[*inds] - perf\n",
    "        perf = R2[*inds]\n",
    "\n",
    "attrs = np.around(np.mean(lifts, axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ff21e-b78b-408d-88df-8efdc36d1a29",
   "metadata": {},
   "source": [
    "### We display the $R^2$ for the model fitted with each subset of the features, and we also display the lift vectors corresponding to each permutation of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900aba7e-cefe-4a36-b2b8-1b61f34ebc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subset | R^2\n",
      "----------------\n",
      "   {}   | 0.0\n",
      "  {1}   | 0.81\n",
      "  {2}   | 0.69\n",
      " {1,2}  | 0.92\n",
      "  {3}   | -0.43\n",
      " {1,3}  | 0.82\n",
      " {2,3}  | 0.69\n",
      "{1,2,3} | 0.92\n"
     ]
    }
   ],
   "source": [
    "print(\"{: ^8}| {}\".format(\"Subset\", \"R^2\"))\n",
    "print(\"----------------\")\n",
    "for n in range(2 ** p):\n",
    "    mask = [n // (2 ** i) % 2 for i in range(p)]\n",
    "    indices = list(itertools.compress(range(p), mask))\n",
    "    S = \"{\" + \"\".join(\"{},\".format(idx + 1) for idx in indices)[:-1] + \"}\"\n",
    "\n",
    "    print(\"{: ^8}| {}\".format(S, R2[*mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aacf446-2ff7-4fc1-bc79-33cac78d3050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation | Lift vector\n",
      "-----------------------------\n",
      "  (1,2,3)   | [0.81 0.11 0.  ]\n",
      "  (1,3,2)   | [0.81 0.1  0.01]\n",
      "  (2,1,3)   | [0.23 0.69 0.  ]\n",
      "  (2,3,1)   | [0.23 0.69 0.  ]\n",
      "  (3,1,2)   | [ 1.25  0.1  -0.43]\n",
      "  (3,2,1)   | [ 0.23  1.12 -0.43]\n"
     ]
    }
   ],
   "source": [
    "print(\"{: <12}| {}\".format(\"Permutation\", \"Lift vector\"))\n",
    "print(\"-----------------------------\")\n",
    "for i, perm in enumerate(perms):\n",
    "    pi = \"(\" + \"\".join(\"{},\".format(p+1) for p in perm)[:-1] + \")\"\n",
    "    print(\"{: ^12}| {}\".format(pi, lifts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aeaf0e-da94-42d7-b4bf-34a3712a46d8",
   "metadata": {},
   "source": [
    "## Comparison of true Shapley attribution and LS-SPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167a1e3-6862-4262-b6a1-2fa3378293ad",
   "metadata": {},
   "source": [
    "### We use LS-SPA to estimate (in one line) the Shapley attribution we computed exactly. We show both for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6b5792-e813-4c0c-94bc-5c7a18bfc346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 19:50:46.140896: W external/xla/xla/service/gpu/nvptx_compiler.cc:673] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.52). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit Shapley attribution: [ 0.59  0.47 -0.14]\n",
      "LS-SPA Shapley attribution:   [ 0.6   0.47 -0.14]\n"
     ]
    }
   ],
   "source": [
    "results = ls_spa(X_train, X_test, y_train, y_test)\n",
    "ls_spa_attrs = np.around(np.array(results.attribution), 2)\n",
    "print(\"Explicit Shapley attribution: {}\".format(attrs))\n",
    "print(\"LS-SPA Shapley attribution:   {}\".format(ls_spa_attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c7546-2e6b-47f3-be26-0e6f63b5cfd3",
   "metadata": {},
   "source": [
    "### We can also print the ShapleyResults object returned by `ls_spa` to see a useful dashboard about the computed Shapley attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93311b5e-8668-4808-b2c9-67406bc532cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        p = 3\n",
      "        Out-of-sample R^2 with all features: 0.92\n",
      "\n",
      "        Shapley attribution: (0.60, 0.47, -0.14)\n",
      "        Estimated error in Shapley attribution: 6.25E-02\n",
      "\n",
      "        Fitted coeficients with all features: (2.07, 1.37, 0.07)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
